+++
title = "Transform data using Glue Studio"
date = 2021-03-17T18:13:14-04:00
weight = 42
chapter = true
+++

In this lab we will use the source tables we created using crawlers in previous lab, to create ETL job using AWS Glue Studio. AWS Glue studio provides visual style of ETL coding, making it simple for ETL developers to write spark code. We will leverage Glue's capability to auto create ETL code to convert source data from CSV to PARQUET format. [Parquet](https://parquet.apache.org/) is an open source, columnar file format which very effective for running analytics queries on distributed data engines.

#### Create ETL job#1
1.  Go to [Glue Studio](https://us-east-2.console.aws.amazon.com/gluestudio/home?region=us-east-2#/) from AWS Management Console or through AWS Glue console and click on Jobs.
    Ensure AWS region is still us-east-2 or Ohio
    {{< img "glue-studio-landing.png" "glue-studio-landing" >}}  
1.1 For the lab our source and target both are on S3, so we will keep the default on Jobs landing page and click Create
    {{< img "click-create-job.png" "click-create-job" >}}  
1.2 You will see the Visual canvas as shown below. Click on first object or node on the visual graph, to reveal its configuration properties on the right.
Input the configuration as shown below
    {{< img "set-data-source1.png" "set-data-source1" >}}  
1.3 Next node is ApplyMapping transform node. Click on it provide details for this node configuration. Click on Choose IAM role button and select the same role you created for Glue crawler lab. After the role is selected it will display the columns mapped. You can take actions like dropping, changing data type etc, but leave it as it is for the lab.
    {{< img "apply-mapping1.png" "apply-mapping1" >}}  
    {{< img "apply-mapping1-role.png" "apply-mapping1-role" >}}  
    {{< img "apply-mapping1-display.png" "apply-mapping1-display" >}}  
1.4 Click on Target node on the graph next. Select Format as Parquet, Compression Type as None and browse and select S3 location as `labdatalake` bucket where data is to be written in parquet.
    {{< img "etl1-target1.png" "etl1-target1" >}} 
We have not created target folder so lets add it to the S3 Target Location path as shown, which Glue will create for us. We should have one folder per target table. Since it is parquet data we will store the data under parquet folder. Append `parquet/enigma_jhu/` to the S3 bucket path you have selected OR you could just paste the whole path as `s3://labdatalake/parquet/enigma_jhu/`
    {{< img "etl1-target1-path.png" "etl1-target1-path" >}}   
1.5 Next click on Script tab to see the script auto generated by Glue Studio. This is a very good way to learn as well.  
1.6 Next click on Job details tab. Put Name as `enigma_jhu_parquet_job`
    Leave everything else as default until you see Job bookmark, select Diable option for that. This is used for incremental jobs, but we are keeping it simple with the lab.
        {{< img "etl1-job-detail1.png" "etl1-job-detail1" >}} 
        {{< img "etl1-job-detail2.png" "etl1-job-detail2" >}}   
1.7 Scroll up and hit Save button to save all the setup  
    {{< img "etl1-job-save.png" "etl1-job-save" >}}  
    This will create the job and you can click Run button to run it
    {{< img "etl1-job-run.png" "etl1-job-run" >}}  
    Click on Run Details from the green banner or Runs tab on the job to see the progress
    {{< img "etl1-job-run-status.png" "etl1-job-run-status" >}}  
    Within couple minutes the job should complete successfully with job status as shown below
    {{< img "etl1-job-run-success.png" "etl1-job-run-success" >}} 
1.8 To check the output on S3, lets go back to Visual tab and click on Target S3 node. Click View next to the target location. It will open up S3 console and the target location for data verification. 
    {{< img "etl1-job-view-data.png" "etl1-job-view-data" >}} 
    {{< img "etl1-job-s3-file.png" "etl1-job-s3-file" >}} 

#### Create ETL job#2
2.  Following above steps, create another Glue Studio ETL job with following changes  
2.1 Data source S3 as Database = `dms_docdb` and Table = `reacr_usa_hospital_beds`  
2.2 ApplyMapping transform node, IAM Role as same role used as before  
2.3 Target S3 node with Format = Parquet and S3 target location as `s3://labdatalake/parquet/hospital_beds/`  
2.4 On Job details tab input following; Name = `hospital_beds_parquet_job`
    IAM Role = `AWSGlueServiceRole-labdatalake`, Job bookmark = `Disable`.  
    Click Save.  
    Once created, click Run.    
2.5 On the Runs tab for the job, ensure it is successful
    {{< img "etl2-job-run-success.png" "etl2-job-run-success" >}}   
2.6 Go back to Visual tab, click on Target S3 node and click on View button, to go to S3 target location to see the parquet file created
    {{< img "etl2-job-s3-file.png" "etl2-job-s3-file" >}}  
