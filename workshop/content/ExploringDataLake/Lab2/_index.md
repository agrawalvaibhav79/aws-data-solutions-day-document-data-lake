+++
title = "Transform data using Glue Studio"
date = 2021-03-17T18:13:14-04:00
weight = 42
chapter = true
+++

In this lab we will use the source tables we created using crawlers in previous lab, to create ETL job using AWS Glue Studio. AWS Glue studio provides visual style of ETL coding, making it simple for ETL developers to write spark code. We will leverage Glue's capability to auto create ETL code to convert source data from CSV to PARQUET format. [Parquet](https://parquet.apache.org/) is an open source, columnar file format which is very effective for running analytics queries.

#### Create ETL job#1
1.  Go to [Glue Studio](https://us-east-2.console.aws.amazon.com/gluestudio/home?region=us-east-2#/) from AWS Management Console or through AWS Glue console and click on Jobs.
    Ensure AWS region is still us-east-2 or Ohio
    {{< img "glue-studio-landing.png" "glue-studio-landing" >}}  
1.1 For the lab our source and target both are on S3, so we will keep the default on Jobs landing page and click Create
    {{< img "click-create-job.png" "click-create-job" >}}  
1.2 You will see the Visual canvas as shown below. Click on first object or node on the visual graph, to reveal its configuration properties on the right.
Input the configuration as shown below
    {{< img "set-data-source1.png" "set-data-source1" >}}  
1.3 Next node is ApplyMapping transform node. Click on it provide details for this node configuration. *(if it asks for - Click on Choose IAM role button and select the same role with **GlueLabRole** in the name as used in Glue crawler lab. For most part you don't need to do it)*. On the Transform tab it will display the source and target columns are mapped. You can take actions like dropping, changing data type etc, but we will leave all defaults for the lab.
     
    {{< img "apply-mapping1-display.png" "apply-mapping1-display" >}}  

{{% notice info %}}
Screenshot is only for illustration purpose, the actual bucket name comes from the Cloudformation Outputs>Bucket name key's value as shown in previous database lab
{{% /notice %}}
1.4 Click on Target node on the graph next. Select Format as Parquet and Compression Type as None. Then browse and select S3 location with **`dmslabs3bucket`** in the bucket name where data is to be written in parquet.
    {{< img "etl1-target1.png" "etl1-target1" >}} 
We have not created target folder so lets add it to the S3 Target Location path as shown, which Glue will create for us when the job runs. We should have one folder per target table. Since it is parquet data we will store the data under parquet folder. Append `parquet/enigma_jhu/` to the S3 bucket path you have selected OR you could just paste the whole path similar to `s3://labdatalake/parquet/enigma_jhu/`
    {{< img "etl1-target1-path.png" "etl1-target1-path" >}}   
1.5 Next click on Script tab to see the script auto generated by Glue Studio. There is option to download and/or edit script manually as well. Although this is a very good way to learn, for this lab we will not do any of that.
1.6 Next click on Job details tab. Put Name as `enigma_jhu_parquet_job`
    Leave everything else as default except
    For IAM Role, pick the role with **GlueLabRole** in the name and for Job bookmark; select Disable option. This is used for incremental jobs, but we are keeping it simple with the lab.
        {{< img "etl1-job-detail1.png" "etl1-job-detail1" >}} 
        {{< img "etl1-job-detail2.png" "etl1-job-detail2" >}}   
1.7 Scroll up and hit Save button to save all the setup  
    {{< img "etl1-job-save.png" "etl1-job-save" >}}  
    This will create the job and you can click Run button to run it
    {{< img "etl1-job-run.png" "etl1-job-run" >}}  
    Click on Run Details from the green banner or Runs tab on the job to see the progress
    {{< img "etl1-job-run-status.png" "etl1-job-run-status" >}}  
    Within couple minutes the job should complete successfully with job status as shown below
    {{< img "etl1-job-run-success.png" "etl1-job-run-success" >}} 
1.8 To check the output on S3, lets go back to Visual tab and click on Target S3 node. Click View next to the target location. It will open up S3 console and the target location for data verification. (or if you are familiar with S3, you can go directly as well)
    {{< img "etl1-job-view-data.png" "etl1-job-view-data" >}} 
    {{< img "etl1-job-s3-file.png" "etl1-job-s3-file" >}} 

#### Create ETL job#2
2.  Following above steps, create another Glue Studio ETL job but with the following information as below -  
2.1 On Data source S3 as Database = `dms_docdb` and Table = `reacr_usa_hospital_beds`  
2.2 On ApplyMapping transform node, check Transform tab as before
2.3 On Target S3 node with Format = Parquet and S3 target location pick the bucket with **`dmslabs3bucket`** in the name and **`parquet`** folder under it. Append to the bucket path a new folder similar to **`hospital_beds/`** . for example the path should look like **`s3://<<dmslabs3>>/parquet/hospital_beds/`**.    
2.4 On Job details tab input following; Name = `hospital_beds_parquet_job`
    IAM Role = `<<GlueLabRole>>`, Job bookmark = `Disable`.  
    Click Save.  
    Once created, click Run.    
2.5 On the Runs tab for the job, ensure it is successful
    {{< img "etl2-job-run-success.png" "etl2-job-run-success" >}}   
2.6 Go back to Visual tab, click on Target S3 node and click on View button, to go to S3 target location to see the parquet file created
    {{< img "etl2-job-s3-file.png" "etl2-job-s3-file" >}}  

So far you have successfully crawled raw data ingested by DMS and created Glue ETL jobs to transform from CSV to PARQUET format. In the next lab you will be able to create another crawler to create data catalog tables for the transformed and processed PARQUET data. For the lab we have a simple ETL scenario, but you may have more complex transformations and there by it is important you crawl and catalog both the raw data and the processed data for downstream availability for analytics.
